{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099e43e-2c7d-42de-9e59-9697ce2b9cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2e6c512-d46a-41bc-9d4b-31fdac1fe1c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Multitasking, Multithreading, and Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698f5a4-4e49-4ae0-a7d9-1a270f69269e",
   "metadata": {},
   "source": [
    "## Tasks, what are they?\n",
    "\n",
    "Julia has the concept of a \"task\", which basically encapsulates some code that executes on the CPU. There is a \"root task\", which is what executes your code initially when you start running your Julia code (in the REPL or from a script), but your code (or code in other Julia libraries) can start extra tasks too. Each task can run different code, and if more than one task exists, they'll each take turns running. If you have more than one Julia task that is ready to run, then Julia will run one task at a time on a given CPU, until that task \"yields\" (sleeps, waits on something to happen, etc.), and then Julia will switch to running another task. This mode of operation is commonly called \"multitasking\" or \"green threading\".\n",
    "\n",
    "To better understand how this works, let's actually spawn some tasks! There are a few ways to spawn tasks - let's use the built-in `@async` macro to create our tasks all on this same CPU (we'll get more adventurous in the next section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112a218e-a435-4506-943b-9ac299499422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 1!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 2!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 3!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 4!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 5!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 1!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 2!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 3!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 4!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 5!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 1!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 2!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 3!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 4!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 5!\n"
     ]
    }
   ],
   "source": [
    "@sync for i in 1:5\n",
    "    @async for j in 1:3\n",
    "        @info \"Hello from task $(i)!\"\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439d01b-5ee6-4899-8fdf-e8231b9b1175",
   "metadata": {},
   "source": [
    "Above, we started 5 tasks with `@async`, and each of these tasks said hello 3 times. We see that, after task 1 says hello, then task 2 says hello, then 3, etc., and then we loop back around. This behavior illustrates that tasks can trade off executing, and so task 2 doesn't have to wait on task 1 to completely finish - task 2 can execute while task 1 still isn't finished running.\n",
    "\n",
    "You'll notice that we put an `@sync` call on the outer loop - why do we do this? Because our tasks run asynchronously, by the time we reach the final `end`, our tasks may not have even started running yet, and Jupyter doesn't wait around for our tasks to finish before showing us what got printed within those loops. What `@sync` does is it waits for all tasks created within the block that it wraps (the outer loop) to complete before continuing on. In this case, it makes sure that all the tasks get the chance to run to completion (and print their output) before letting Jupyter send that output to our screen. It's something of a minor technical detail here, but this asynchronous behavior (and the need to \"synchronize\" like this) is key to multitasking in general.\n",
    "\n",
    "This pattern we see above is good for \"embarrasingly parallel\" algorithms, where our tasks don't need to share data or depend on each other (they're fully independent). When our algorithms need to share data or communicate, we have tools available to help. In particular, tasks can wait on each other to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d6c3cf-71e5-44ad-8d4c-a74409cda637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I happen first\n",
      "I happen second\n"
     ]
    }
   ],
   "source": [
    "t1 = @async println(\"I happen first\")\n",
    "t2 = @async begin\n",
    "    wait(t1)\n",
    "    println(\"I happen second\")\n",
    "end\n",
    "wait(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9da20-f9aa-4c00-93e2-8d20874bd838",
   "metadata": {},
   "source": [
    "Here we see that task 2 waits on task 1 to finish before it prints, so in effect, we see task 1's println before we see task 2's println.\n",
    "\n",
    "But furthermore, tasks also have return values (just like normal Julia functions), and we can see the return value of a task by fetching it (which also waits for the task to finish):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "557e92ee-215a-48c2-99b6-c237a3d80e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = @async 1+1\n",
    "t2 = @async fetch(t1)*3\n",
    "fetch(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bd4c9-ac02-49e8-b5cf-70d02dd321f5",
   "metadata": {},
   "source": [
    "Here, task 2 uses the result of task 1 to compute its result, so task 2 depends on task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba388e-ce12-499c-a91d-0a646de29f3e",
   "metadata": {},
   "source": [
    "We can take this further by making a task fetch from multiple other tasks, or having a task's result be propagated to multiple other \"downstream\" tasks that might need it. By building up a \"graph\" of tasks which depend on each other (and synchronize and communicate via `wait`, `fetch`, or other mechanisms like `Channel`s), we can build very \"concurrent\" programs (\"concurrent\" really just means \"many independent things running in tandem or simultaneously). This kind of \"multitasking\" paradigm is really powerful for building complex programs, like data analysis pipelines, web servers, spreadsheets, and much, much more. However, the model as presented is limited in how far it can scale: everything we've seen so far only runs on a single CPU, and modern computers typically have many more than one CPU available. What if we want to be able to make use of these extra CPUs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eeed3a-3f29-4e7c-bcf2-ea86da837df1",
   "metadata": {},
   "source": [
    "## Gotta go fast\n",
    "\n",
    "If you have multiple CPUs, you can tell Julia to start with multiple threads (like with `julia -t6` to start Julia with 6 threads), and then Julia can run multiple tasks at the same time, by putting a different task on each thread (which will then each run on a different CPU). This is commonly called \"multithreading\", and is probably the most common mechanism that users and libraries will use to accelerate algorithms. Of course, not all algorithms support multithreaded execution - algorithms which can't be split up into many separate tasks are called \"sequential\" or \"serial\". While all algorithms have some amount of sequential behavior, many algorithms can be \"parallelized\" by running portions of them independently. Julia's multithreading facilities make it easy to parallelize algorithms when it's possible to do so.\n",
    "\n",
    "Let's first see what it looks like to use multithreading, with Julia's `Threads.@spawn` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1eb83f-4385-40af-982e-73ba24f55e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 4!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 2!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 3!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 1!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 5!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 1!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 1!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 4!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 3!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 2!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 5!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 3!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 4!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 2!\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mHello from task 5!\n"
     ]
    }
   ],
   "source": [
    "@sync for i in 1:5\n",
    "    Threads.@spawn for j in 1:3\n",
    "        @info \"Hello from task $(i)!\"\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9d7bc-ad62-436c-ae45-f6696301b8c7",
   "metadata": {},
   "source": [
    "This doesn't look really much different from `@async` - things are a little bit more out-of-order, but we still see each value of `i` printed 3 times. That our tasks are running on different threads can only really be seen by printing which thread our task is actively running on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25930e0-7d0e-483d-8281-3d5118459692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTask 3 running on thread 1\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTask 4 running on thread 4\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTask 2 running on thread 2\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTask 5 running on thread 3\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTask 1 running on thread 6\n"
     ]
    }
   ],
   "source": [
    "@sync for i in 1:5\n",
    "    Threads.@spawn @info \"Task $(i) running on thread $(Threads.threadid())\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f5d6c-0f67-4634-b034-8c1930fc03f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now we see that our tasks are all running on different threads (which each will run on a different CPU core). Everything works otherwise the same as `@async` - tasks can wait on each other, fetch each other's results, etc., but they also happen to be able to do this while running on different threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa2cc5-4c42-4ca6-9d89-357a8c9fe569",
   "metadata": {},
   "source": [
    "## I need more Julias!\n",
    "\n",
    "If you're lucky enough to have multiple servers at your disposal, you might be wondering if you can parallelize code across those servers. You indeed can, and it can look quite similar to what we can do with multithreading, with a few key caveats that we'll discuss soon. We can extend the idea of Julia's tasks to multiple servers by using Julia's Distributed standard library, which makes it easy to run tasks on remote servers. Let's setup some workers locally, just to see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06c7f7-c925-4fee-a2ac-2c9475d0fdcb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Distributed\n",
    "\n",
    "# Let's start up some local workers\n",
    "if VERSION >= v\"11-\"\n",
    "    # If you're using Julia >=1.11, then we can safely use multiple threads\n",
    "    addprocs(;exeflags=[\"--project=$(pwd())\", \"--threads=2\"])\n",
    "elseif Threads.nthreads() == 1\n",
    "    # Julia <1.11 cannot safely mix multiple threads and Distributed workers\n",
    "    addprocs(;exeflags=\"--project=$(pwd())\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d80f08-c752-485e-98e3-e3077ce3b477",
   "metadata": {},
   "source": [
    "Note that we're only adding workers on our local system - this keeps it simpler for everyone at the workshop today, but you can always connect to actual remote servers with `addprocs`, and everything will work basically the same! Additionally, if you're not using Julia 1.11 or higher, there are some bugs within Distributed that make it unsafe to use when Julia is running with multiple threads, so we won't start any workers here. But you can follow along all the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a03b4-3e24-445d-b3b3-3f55f5537a1a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0-beta2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
